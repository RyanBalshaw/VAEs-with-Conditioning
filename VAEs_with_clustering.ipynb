{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAEs_with_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/S8+bWeuhKxBjiN3iALfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanBalshaw/VAEs-with-Conditioning/blob/main/VAEs_with_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGxPR-DJgWha"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFlOlkXqvj2Z"
      },
      "source": [
        "VaDE versus CURL:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The differences are minor, you can easily show that the two are the same. Write this up at some point for reference.\n",
        "\n",
        "VaDE objective function:\n",
        "\\begin{equation}\n",
        "L_{VaDE} = \\mathbb{E}_{q(\\mathbf{z}\\vert\\mathbf{x})}\\left[ \\log p(\\mathbf{x}\\vert\\mathbf{z}) \\right] - \\mathbb{E}_{q(\\mathbf{y}\\vert\\mathbf{x})}\\left[ KL(q(\\mathbf{z}\\vert\\mathbf{x}))\\Vert p(\\mathbf{z}\\vert \\mathbf{y}))\\right] - KL(q(\\mathbf{y}\\vert\\mathbf{x})\\Vert p(\\mathbf{y}))\n",
        "\\end{equation}\n",
        "\n",
        "CURL objective function:\n",
        "\\begin{equation}\n",
        "L_{CURL} = \\mathbb{E}_{q(\\mathbf{z}\\vert\\mathbf{x}, \\mathbf{y})q(\\mathbf{y}\\vert\\mathbf{x})}\\left[ \\log p(\\mathbf{x}\\vert\\mathbf{z}) \\right] - \\mathbb{E}_{q(\\mathbf{y}\\vert\\mathbf{x})}\\left[ KL(q(\\mathbf{z}\\vert\\mathbf{x}, \\mathbf{y}))\\Vert p(\\mathbf{z}\\vert \\mathbf{y}))\\right] - KL(q(\\mathbf{y}\\Vert\\mathbf{x})\\vert p(\\mathbf{y}))\n",
        "\\end{equation}\n",
        "\n",
        "Important notes:\n",
        "- Decoder: $p(\\mathbf{x}\\vert\\mathbf{z}) \\sim \\mathcal{N}(\\mathbf{x}\\vert \\mathbf{\\mu}(\\mathbf{z}), \\mathbf{\\sigma}^2(\\mathbf{z})\\mathbf{I})$\n",
        "- Encoder: $q_{VaDE}(\\mathbf{z}\\vert \\mathbf{x}) \\sim \\mathcal{N}(\\mathbf{z}\\vert \\mathbf{\\mu}_{\\mathbf{z}}(\\mathbf{x}), \\mathbf{\\sigma}_{\\mathbf{z}}^2(\\mathbf{x})\\mathbf{I})$ OR $q_{CURL}(\\mathbf{z}\\vert \\mathbf{x}, \\mathbf{y}) \\sim \\mathcal{N}(\\mathbf{z}\\vert \\mathbf{\\mu}_{\\mathbf{z}}(\\mathbf{x}, \\mathbf{y}), \\mathbf{\\sigma}_{\\mathbf{z}}^2(\\mathbf{x}, \\mathbf{y})\\mathbf{I})$\n",
        "- Prior $p(\\mathbf{z}\\vert\\mathbf{y}) \\sim \\mathcal{N}(\\mathbf{z}\\vert \\mathbf{\\mu}_{\\mathbf{z}}(\\mathbf{y}), \\mathbf{\\sigma}_{\\mathbf{z}}^2(\\mathbf{y})\\mathbf{I}))$\n",
        "\n",
        "- Prior $p(\\mathbf{y}) \\sim Cat(\\mathbf{\\pi})$ = $\\prod_{k=1}^{K}\\pi_k^{z_k}$\n",
        "\n",
        "The final component is to define $q(\\mathbf{y}\\vert\\mathbf{x})$. VaDE and CURL take vastly different approaches:\n",
        "\n",
        "for VaDE:\n",
        "$q(\\mathbf{y}\\vert\\mathbf{x}) = p(\\mathbf{y}\\vert\\mathbf{z}) = \\frac{p(\\mathbf{y})p(\\mathbf{z}\\vert\\mathbf{y})}{\\sum_{i=1}^Kp(\\mathbf{y})p(\\mathbf{z}\\vert\\mathbf{y})},$\n",
        "\n",
        "where this equation also features in linear mixture models. This term is the posterior probability for y given an observation.\n",
        "\n",
        "For CURL:\n",
        "$q(\\mathbf{y}\\vert\\mathbf{x})$ is part of the encoder, with a softmax 'task inference' head. I like this formulation a little less as I am not convinced that it works well.\n",
        "\n",
        "Why do I say this? Well I noted one potentially problematic area in how CURL estimates the 'categorical regulariser'. From their code, they take a batch and take the average of the argmax of the labels $\\mathbf{y}$, essentially estimating the batch class likelihood. The problem here is that it is not given that a batch will have equal samples from each 'hidden class', so I am not sure how useful this will be when there are unequal spread in the classes. Maybe you need to perform some pre-training inference for the prior $p(\\mathbf{y})$.\n",
        "\n",
        "It is natural, for the continuous distribution $q(\\mathbf{z}\\vert\\cdots)$, to take a Monte Carlo estimate. However, the addition of the distribution categorial distribution, any expectation over $q(\\mathbf{y}\\vert\\mathbf{x})$ then becomes a summation over K indices of the term in the expectation weighted by $q(\\mathbf{y}=i\\vert\\mathbf{x})$.\n",
        "\n",
        "Furthermore, for the middle KL divergence term, both distributions in the KL divergence are Gaussian and thus the KL divergence becomes:\n",
        "$KL(\\mathcal{N}_0\\Vert\\mathcal{N}_1) = \\frac{1}{2}\\left( tr(\\Sigma_1^{-1}\\Sigma_0) + (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_0)^T\\Sigma_1^{-1}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_0) - k + \\log\\left(\\frac{det\\Sigma_1}{det\\Sigma_2}\\right) \\right)$,\n",
        "\n",
        "where $\\mathcal{N}_0\\sim\\mathcal{N}(\\mathbf{\\mu}_0, \\Sigma_0)$, $\\mathcal{N}_1\\sim\\mathcal{N}(\\mathbf{\\mu}_1, \\Sigma_1)$ and $k$ is the dimensionality of the space covered by the distribution. If $\\Sigma_1$ and $\\Sigma_2$ are parametrised as diagonal covariance distributions $\\Sigma_0 = \\mathbf{\\sigma_0^2}\\mathbf{I}$ and $\\Sigma_1 = \\mathbf{\\sigma_1^2}\\mathbf{I}$ then\n",
        "\n",
        "$KL(\\mathcal{N}_0\\Vert\\mathcal{N}_1) = \\frac{1}{2}\\left( \\sum_{i} \\frac{\\sigma^2_{0,i}}{\\sigma^2_{1,i}} + \\sum_i\\left(\\frac{\\mu_{1,i} - \\mu_{0,i})^2}{\\sigma^2_{1,i}}\\right)  - k + \\sum_i \\log\\left(\\frac{\\sigma^2_{1, i}}{\\sigma^2_{0, i}}\\right)  \\right)$,\n",
        "$KL(\\mathcal{N}_0\\Vert\\mathcal{N}_1) = \\frac{1}{2}\\sum_{i}\\left(  \\frac{\\sigma^2_{0,i}}{\\sigma^2_{1,i}} + \\left(\\frac{\\mu_{1,i} - \\mu_{0,i})^2}{\\sigma^2_{1,i}}\\right)  - 1 +  \\log\\left(\\frac{\\sigma^2_{1, i}}{\\sigma^2_{0, i}}\\right)  \\right)$,\n",
        "\n",
        "Finally, the final KL divergence term can be expanded as follows:\n",
        "$=\\sum_{k=1}^K q(\\mathbf{y}=k\\vert\\mathbf{x}) \\log \\left( \\frac{q(\\mathbf{y}=k\\vert\\mathbf{x})}{p(\\mathbf{y}=k)} \\right)$\n",
        "\n",
        "\n",
        "Let's now focus on the final KL term $KL(q(\\mathbf{y}\\vert\\mathbf{x})\\Vert p(\\mathbf{y}))$. Since we know that this regularises the posterior conditional probability (i.e. the conditional probability given a sample from $\\mathbf{x}$, which is actually a $\\mathbf{z}$ if you think about where the MoG lies), we need a method to evaluate the KL term. The expansion of the term is straightforward:\n",
        "\n",
        "$KL(q(\\mathbf{y}\\vert\\mathbf{x})\\Vert p(\\mathbf{y})) = \\mathbb{E}_{q(\\mathbf{y}\\vert\\mathbf{x}}[q(\\mathbf{y}\\vert\\mathbf{x})\\left( \\log \\frac{q(\\mathbf{y}\\vert\\mathbf{x})}{p(\\mathbf{y})} \\right)]$\n",
        "\n",
        "and since y is discrete:\n",
        "\n",
        "$KL(q(\\mathbf{y}\\vert\\mathbf{x})\\Vert p(\\mathbf{y})) = \\sum_{k=1}^{K}q(\\mathbf{y}=k\\vert\\mathbf{x})\\left( \\log \\frac{p(\\mathbf{y}=k\\vert\\mathbf{x})}{p(\\mathbf{y}=k)} \\right) = \\sum_{k=1}^{K}q(\\mathbf{y}=k\\vert\\mathbf{x})\\left( \\log \\frac{p(\\mathbf{y}=k\\vert\\mathbf{x})}{\\pi_k} \\right)$.\n",
        "\n",
        "So, what do CURL and VaDE do? VaDE tries to estimate $p(\\mathbf{y}=k\\vert\\mathbf{x})$ for each $\\mathbf{x}$ while CURL uses a batch-estimated posterior and effectively directly parametrises $p(\\mathbf{y}\\vert\\mathbf{x}) = \\prod_{k=1}^{K} \\gamma_{k}^{y_k}$, where $\\gamma_{k}$ is the batch estimated class likelihood. \n",
        "\n",
        "Personally, I like what VaDE does more, but it is something to test out.\n",
        "\n",
        "I just realised there is an alternative derivation, one which allows one to use a cross-entropy term. Let's dissect the KL term even more: since we know $\\pi_k$ is a constant scalar, we can separate the terms nicely:\n",
        "\n",
        "$ KL(q(\\mathbf{y}\\vert\\mathbf{x})\\Vert p(\\mathbf{y})) = \\sum_{k=1}^{K}q(\\mathbf{y}=k\\vert\\mathbf{x})\\left( \\log p(\\mathbf{y}=k\\vert\\mathbf{x}) \\right) -  \\sum_{k=1}^{K}q(\\mathbf{y}=k\\vert\\mathbf{x})\\log \\left( pi_k \\right) $\n",
        "\n",
        "where the term on the right can be grouped with the $q(\\mathbf{y}\\vert\\mathbf{x})$ terms in the previous objective functions. This leaves us with $\\sum_{k=1}^{K}q(\\mathbf{y}=k\\vert\\mathbf{x})\\left( \\log p(\\mathbf{y}=k\\vert\\mathbf{x}) \\right)$. What do we do with this? Well if we think about it a little, this is simply the entropy $\\mathcal{H}(q(\\mathbf{y}\\vert\\mathbf{x}))$, which is a cross-entropy loss. However, the difference is that we do not have labels and the labels, although they should be 1-of-K, will not be. Thus, it is better to leave it as the entropy. I believe Tensorflows softmax_cross_entropy_with_logits is better suited.\n",
        "\n",
        "At the end of the day, I think simply taking the loss at face value is the way to proceed, monitoring $\\mathcal{H}(q(\\mathbf{y}\\vert\\mathbf{x}))$ will be useful as this will tell us if any information is encoded into $\\mathbf{y}$, or if it is just left as is by the model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaLM8vRjkG-m"
      },
      "source": [
        "def apply_MLP_to_source(source,\n",
        "                        num_layer,\n",
        "                        num_segment = None,\n",
        "                        iter4condthresh = 10000,\n",
        "                        cond_thresh_ratio = 0.25,\n",
        "                        layer_name_base = 'ip',\n",
        "                        save_layer_data = False,\n",
        "                        Arange=None,\n",
        "                        nonlinear_type = 'ReLU',\n",
        "                        negative_slope = 0.2,\n",
        "                        random_seed=0):\n",
        "    \"\"\"Generate MLP and Apply it to source signal.\n",
        "    Args:\n",
        "        source: source signals. 2D ndarray [num_comp, num_data]\n",
        "        num_layer: number of layers\n",
        "        num_segment: (option) number of segments (only used to modulate random_seed)\n",
        "        iter4condthresh: (option) number of random iteration to decide the threshold of condition number of mixing matrices\n",
        "        cond_thresh_ratio: (option) percentile of condition number to decide its threshold\n",
        "        layer_name_base: (option) layer name\n",
        "        save_layer_data: (option) if true, save activities of all layers\n",
        "        Arange: (option) range of value of mixing matrices\n",
        "        nonlinear_type: (option) type of nonlinearity\n",
        "        negative_slope: (option) parameter of leaky-ReLU\n",
        "        random_seed: (option) random seed\n",
        "    Returns:\n",
        "        mixedsig: sensor signals. 2D ndarray [num_comp, num_data]\n",
        "        mixlayer: parameters of mixing layers\n",
        "    \"\"\"\n",
        "    if Arange is None:\n",
        "        Arange = [-1, 1]\n",
        "    #print(\"Generating sensor signal...\")\n",
        "\n",
        "    # Subfuction to normalize mixing matrix\n",
        "    def l2normalize(Amat, axis=0):\n",
        "        # axis: 0=column-normalization, 1=row-normalization\n",
        "        l2norm = np.sqrt(np.sum(Amat*Amat,axis))\n",
        "        Amat = Amat / l2norm\n",
        "        return Amat\n",
        "\n",
        "    # Initialize random generator\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # To change random_seed based on num_layer and num_segment\n",
        "    for i in range(num_layer):\n",
        "        np.random.rand()\n",
        "\n",
        "    if num_segment is not None:\n",
        "        for i in range(num_segment):\n",
        "            np.random.rand()\n",
        "\n",
        "    num_comp = source.shape[0]\n",
        "\n",
        "    # Determine condThresh ------------------------------------\n",
        "    condList = np.zeros([iter4condthresh])\n",
        "    \n",
        "    for i in range(iter4condthresh):\n",
        "        A = np.random.uniform(Arange[0],Arange[1],[num_comp,num_comp])\n",
        "        A = l2normalize(A, axis=0)\n",
        "        condList[i] = np.linalg.cond(A)\n",
        "\n",
        "    condList.sort() # Ascending order\n",
        "    condThresh = condList[int(iter4condthresh * cond_thresh_ratio)]\n",
        "    #print(\"    cond thresh: {0:f}\".format(condThresh))\n",
        "\n",
        "    # Generate mixed signal -----------------------------------\n",
        "    mixedsig = source.copy()\n",
        "    mixlayer = []\n",
        "    for ln in range(num_layer-1,-1,-1):\n",
        "\n",
        "        # Apply nonlinearity ----------------------------------\n",
        "        if ln < num_layer-1: # No nolinearity for the first layer (source signal)\n",
        "            if nonlinear_type == \"ReLU\": # Leaky-ReLU\n",
        "                mixedsig[mixedsig<0] = negative_slope * mixedsig[mixedsig<0]\n",
        "            else:\n",
        "                raise ValueError\n",
        "\n",
        "        # Generate mixing matrix ------------------------------\n",
        "        condA = condThresh + 1\n",
        "        while condA > condThresh:\n",
        "            A = np.random.uniform(Arange[0], Arange[1], [num_comp, num_comp])\n",
        "            A = l2normalize(A)  # Normalize (column)\n",
        "            condA = np.linalg.cond(A)\n",
        "            #print(\"    L{0:d}: cond={1:f}\".format(ln, condA))\n",
        "        # Bias\n",
        "        b = np.zeros([num_comp]).reshape([1,-1]).T\n",
        "\n",
        "        # Apply bias and mixing matrix ------------------------\n",
        "        mixedsig = mixedsig + b\n",
        "        mixedsig = np.dot(A, mixedsig)\n",
        "\n",
        "        # Storege ---------------------------------------------\n",
        "        layername = layer_name_base + str(ln+1)\n",
        "        mixlayer.append({\"name\":layername, \"A\":A.copy(), \"b\":b.copy()})\n",
        "        # Storege data\n",
        "        if save_layer_data:\n",
        "            mixlayer[-1][\"x\"] = mixedsig.copy()\n",
        "\n",
        "    return mixedsig, mixlayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ8e7-cmkKou"
      },
      "source": [
        "# Datasets\n",
        "class mixing_MLP(nn.Module): #Depreciated, no longer used.\n",
        "    def __init__(self, n_size, n_layers):\n",
        "        super(mixing_MLP, self).__init__()\n",
        "        \n",
        "        self.layers = []\n",
        "        self.activation = nn.LeakyReLU(negative_slope = 0.5)\n",
        "        \n",
        "        for i in range(n_layers):\n",
        "            self.layers.append(nn.Linear(n_size, n_size))\n",
        "            self.layers.append(self.activation)\n",
        "        \n",
        "        #self.layers.pop(-1)\n",
        "        \n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "        self.model.apply(self.init_weights)\n",
        "    \n",
        "    @staticmethod\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            return self.model(x)\n",
        "        \n",
        "class iVAE_datasets(object):\n",
        "    \n",
        "    def __init__(self, n, M, Lsegments, k, batch_size = 64, randomise = True, random_seed = False, mod_flag = False, mix_L = 1, Gauss_source = True, seed = True):\n",
        "        \"\"\"\n",
        "        n = size of latent space\n",
        "        M = no. classes\n",
        "        Lsegments = no. samples per class\n",
        "        k = no. of prior parameters\n",
        "            k = 1: variance Gaussian\n",
        "            k = 2: mean and variance gaussian\n",
        "        mod_flag = case where one signal has mean modulation and the other doesn't\n",
        "        \"\"\"\n",
        "        self.latent_size = n\n",
        "        self.no_classes = M\n",
        "        self.no_samples = Lsegments\n",
        "        self.k = k\n",
        "        self.batch_size = batch_size #specifies the batch size\n",
        "        self.randomise = randomise #Specifies that sample must be obtained randomly (not uniformly)\n",
        "        self.random_seed = random_seed #If random_seed = True - specifies that a random sample is required and the counter is not increased!\n",
        "        self.mod_flag = mod_flag\n",
        "        self.mix_L = mix_L\n",
        "        self.Gauss_source = Gauss_source\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        #Define mixing model (Unused)\n",
        "        #self.mixing_model = mixing_MLP(self.latent_size, 1)\n",
        "        #self.mixing_model.to(self.device)\n",
        "        #print(self.mixing_model)\n",
        "        \n",
        "        if self.k == 1 and not self.mod_flag:\n",
        "            #print(\"\\nVariance modulated sources.\\n\")\n",
        "            self.mu_centers = np.zeros((self.no_classes, self.latent_size))\n",
        "            \n",
        "        elif self.k == 2 and not self.mod_flag:\n",
        "            #print(\"\\nMean and variance modulated sources.\\n\")\n",
        "            if seed:\n",
        "                np.random.seed(2**13 + 4)\n",
        "\n",
        "            self.mu_centers = np.random.rand(self.no_classes, self.latent_size) * 10 - 5         # in range (-5, 5)\n",
        "            #self.mu_centers += np.sign(self.mu_centers) * 0.5 #Shift centers a outwards a little\n",
        "       \n",
        "        else:\n",
        "            self.mu_centers = np.zeros((self.no_classes, self.latent_size))\n",
        "            \n",
        "            list_range = np.arange(0, self.no_classes, 1)\n",
        "            np.random.shuffle(list_range) #random permutation gamma(u)\n",
        "            \n",
        "            a = np.random.random() * 10 - 5\n",
        "            \n",
        "            self.mu_centers[:, 1] =  a * list_range\n",
        "            \n",
        "        if not hasattr(self, \"std_centers\"):\n",
        "            self.std_centers = np.random.rand(self.no_classes, self.latent_size) * 2.5 + 0.5      # in range (0.5, 3)\n",
        "        \n",
        "        #Make the sample labels\n",
        "        self.sample_labels = []\n",
        "        for i in range(self.no_classes):\n",
        "            self.sample_labels += [i] * self.no_samples\n",
        "        self.sample_labels = np.array(self.sample_labels)\n",
        "\n",
        "        self.data = torch.from_numpy(self.sample()).to(self.device)\n",
        "        \n",
        "        #Normalise      \n",
        "        self.data_mean = torch.mean(self.data, dim = 0)\n",
        "        self.data_std = torch.std(self.data, dim = 0)\n",
        "        \n",
        "        #self.data = (self.data - self.data_mean) / self.data_std\n",
        "\n",
        "        mixed_data, mix_layer = apply_MLP_to_source(self.data.cpu().numpy().T, self.mix_L, num_segment = self.no_classes)\n",
        "        mixed_data = torch.from_numpy(mixed_data.T).float()\n",
        "\n",
        "        self.mix_layer = mix_layer \n",
        "\n",
        "        if self.mod_flag:\n",
        "            self.mixed_data = torch.hstack((mixed_data[:, [0]], self.data[:, [1]]))\n",
        "        \n",
        "        else:\n",
        "            self.mixed_data = mixed_data\n",
        "        \n",
        "        #Add noise\n",
        "        self.mixed_data += torch.randn_like(self.mixed_data) * 0.01\n",
        "        \n",
        "        self.data_tuples = list(zip(self.data, self.sample_labels)) #list of tuples\n",
        "        self.mixed_data_tuples = list(zip(self.mixed_data, self.sample_labels)) #list of tuples\n",
        "\n",
        "        #shuffle mixed_data\n",
        "        self.shuffled_data_index = np.arange(0, self.mixed_data.size(0), 1, dtype = int)\n",
        "\n",
        "        if self.random_seed:\n",
        "            np.random.shuffle(self.shuffled_data_index)\n",
        "\n",
        "        #Convert self.sample_labels to torch.tensor\n",
        "        self.sample_labels = torch.from_numpy(self.sample_labels)\n",
        "    \n",
        "    def sample(self):\n",
        "        selected_centers = self.sample_labels\n",
        "        \n",
        "        latent_sample = self.mu_centers[selected_centers, :]\n",
        "\n",
        "        if self.Gauss_source:\n",
        "            latent_sample += np.random.randn(len(selected_centers), self.latent_size) * self.std_centers[selected_centers, :]\n",
        "        elif not self.Gauss_source and self.latent_size == 2:\n",
        "            s1 = np.random.laplace(loc = 0, scale = self.std_centers[selected_centers, 0]).reshape(-1, 1)\n",
        "            s2 = np.random.laplace(loc = 0, scale = self.std_centers[selected_centers, 1]).reshape(-1, 1)\n",
        "            latent_sample += np.hstack((s1, s2))\n",
        "        return latent_sample.astype(np.float32)\n",
        "    \n",
        "    #turn the class into an iterator\n",
        "    def __iter__(self):\n",
        "        \n",
        "        self.iter_cnt = 0 #initialises the iterator\n",
        "        return self #returns the iterator object\n",
        "    \n",
        "    def __next__(self):\n",
        "\n",
        "        if not self.random_seed:\n",
        "            start = self.iter_cnt * self.batch_size\n",
        "            end = start + self.batch_size\n",
        "\n",
        "            index = self.shuffled_data_index[start:end]\n",
        "\n",
        "            if end <= len(self.mixed_data_tuples):\n",
        "\n",
        "                self.iter_cnt += 1\n",
        "                \n",
        "                data = self.mixed_data[index, :]\n",
        "                labels = self.sample_labels[index]\n",
        "                \n",
        "                return data, labels\n",
        "\n",
        "            else:\n",
        "                self.iter_cnt= 0\n",
        "                raise StopIteration\n",
        "\n",
        "        else:\n",
        "              print(\"Random sampler is not implemented.\")\n",
        "              raise SystemExit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgXKzr5OWMi"
      },
      "source": [
        "# Objective functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "UPuk89DsOVsa",
        "outputId": "f46a8fc5-fb36-4fea-beea-48f02d4c53de"
      },
      "source": [
        "class GaussianLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GaussianLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, x, x_recon, sum_vals = True):\n",
        "\n",
        "        if isinstance(x_recon, tuple):\n",
        "            #Learnt a variance on the output\n",
        "            mu_recon, var_recon = x_recon\n",
        "\n",
        "        else:\n",
        "            #No learnt variance on the output\n",
        "            mu_recon = x_recon\n",
        "            var_recon = torch.ones_like(x_recon).requires_grad_(False)\n",
        "        \n",
        "        error = x - mu_recon\n",
        "\n",
        "        B, N = x.size()\n",
        "        #Assuming diagonalised covariance:\n",
        "        gauss_log_loss = torch.mul(error.pow(2), 1/(2 * var_recon + 1e-12)) #2x100 error vector is needed to do normal multiplication\n",
        "        gauss_log_loss += 1/2 * torch.log(var_recon + 1e-12)\n",
        "\n",
        "        #Sum over dimensionality\n",
        "        gauss_log_loss = torch.sum(gauss_log_loss, dim = 1, keepdim = True)\n",
        "        \n",
        "        if sum_vals:\n",
        "            gauss_log_loss +=  torch.sum(gauss_log_loss)\n",
        "\n",
        "        return gauss_log_loss #Unnormalised\n",
        "\n",
        "\n",
        "class KL_divergence(nn.Module):\n",
        "    def __init__(self, std_normal = False):\n",
        "        super(KL_divergence, self).__init__()\n",
        "\n",
        "        self.std_normal = std_normal #A flag to check whether the loss\n",
        "\n",
        "    def forward(self, mu_0, var_0, mu_1 = None, var_1 = None):\n",
        "\n",
        "        if self.std_normal:\n",
        "            mu_1 = torch.zeros_like(mu_0).requires_grad_(False)\n",
        "            var_1 = torch.ones_like(var_0).requires_grad_(False)\n",
        "\n",
        "        #perform everything elementwise and then \n",
        "        Dkl = var_0 / var_1 + ((mu_1 - mu_0)**2) / var_1 - 1 + torch.log(var_1 / var_0)\n",
        "\n",
        "        #Sum over dimensionality\n",
        "        Dkl = 0.5 * torch.sum(Dkl, dim = 1, keepdim = True)\n",
        "\n",
        "        return Dkl #Unnormalised\n",
        "\n",
        "class discrete_KL_divergence(nn.Module):\n",
        "    def __init__(self, no_classes):\n",
        "        super(discrete_KL_divergence, self).__init__()\n",
        "\n",
        "        self.no_classes = no_classes\n",
        "        self.uniform_prior = uniform_prior\n",
        "\n",
        "\n",
        "    def forward(self, class_prob, prior_prob = None):\n",
        "\n",
        "        assert class_prob.size(1) == self.no_classes, \"There is a mis-match between the pre-defined number of classes and the number of classes given to the discrete KL divergence.\"\n",
        "\n",
        "        if prior_prob is None:\n",
        "          prior_prob = torch.ones(self.no_classes) / self.no_classes\n",
        "\n",
        "        #perform everything elementwise and then \n",
        "        Dkl = class_prob * (torch.log(class_prob) - torch.log(self.prior_prob))\n",
        "\n",
        "        return Dkl #Unnormalised\n",
        "\n",
        "class VAE_loss(nn.Module):\n",
        "    #No ability to learn a variance, variance is controlled by the noise distribution for iVAE\n",
        "    def __init__(self, loss_name = \"L2\", gamma = 1, beta = 1, std_normal = False):\n",
        "        super(VAE_loss, self).__init__()\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "\n",
        "        self.loss_name = loss_name\n",
        "\n",
        "        if self.loss_name.lower() == \"l2\":\n",
        "            self.recon_loss = GaussianLoss()\n",
        "        \n",
        "        elif self.loss_name.lower() == \"l1\":\n",
        "            self.recon_loss = nn.L1Loss(size_average = False) #Turn off averaging by size as you do it by yourself at the end.\n",
        "        \n",
        "        else:\n",
        "            print(\"Unknown loss entered.\")\n",
        "            raise SystemExit\n",
        "        \n",
        "\n",
        "        self.kl_loss = KL_divergence(std_normal)\n",
        "    \n",
        "    def forward(self, x, recon_x, mu_0, var_0, mu_1 = None, var_1 = None):\n",
        "        \n",
        "        B, N = x.size()\n",
        "\n",
        "        if isinstance(recon_x, tuple) and self.loss_name.lower() == \"l1\": #Check if it is a tuple, will be this by default when it is fed in.\n",
        "            recon_x = recon_x[0]\n",
        "\n",
        "        Lrecon = self.recon_loss(x, recon_x)\n",
        "        Lkl = self.kl_loss(mu_0, var_0, mu_1, var_1) #Need to normalise with same values reconstruction loss (Pytorch does this automatically unless you specify)\n",
        "        \n",
        "        Ltotal = torch.sum(self.gamma * Lrecon + self.beta * Lkl)\n",
        "\n",
        "        #Normalise value\n",
        "        Lrecon /= (B * N)\n",
        "        Lkl_continuous /= (B * N)\n",
        "        Ltotal /= (B * N)\n",
        "\n",
        "        return Ltotal, Lrecon, Lkl\n",
        "\n",
        "class MoG_VAE_loss(nn.Module):\n",
        "    #No ability to learn a variance, variance is controlled by the noise distribution for iVAE\n",
        "    def __init__(self, no_classes, loss_name = \"L2\", gamma = 1, beta = 1, alpha = 1):\n",
        "        super(MoG_VAE_loss, self).__init__()\n",
        "\n",
        "        self.no_classes = no_classes\n",
        "\n",
        "        self.gamma = gamma #Reconstruction loss KL parameter\n",
        "        self.beta = beta #Continuous KL parameter\n",
        "        self.alpha = alpha #Categorial KL parameter\n",
        "\n",
        "        self.loss_name = loss_name\n",
        "\n",
        "        if self.loss_name.lower() == \"l2\":\n",
        "            self.recon_loss = GaussianLoss()\n",
        "        \n",
        "        elif self.loss_name.lower() == \"l1\":\n",
        "            self.recon_loss = nn.L1Loss()\n",
        "        \n",
        "        else:\n",
        "            print(\"Unknown loss entered.\")\n",
        "            raise SystemExit\n",
        "        \n",
        "        self.kl_loss = KL_divergence(False) #Never use a standard VAE case\n",
        "        self.discrete_kl_loss = discrete_KL_divergence(self.no_classes, uniform_prior = True) #assume a uniform prior\n",
        "    \n",
        "    def forward(self, x, recon_x, mu_0, var_0, mu_1, var_1, q_y_G_x, prior_prob = None, CURL = False):\n",
        "        \n",
        "        #You need to expand the input data by no_classes and then reshape it!\n",
        "\n",
        "        kB, N = x.size()\n",
        "        B = kB // self.no_classes\n",
        "\n",
        "        if isinstance(recon_x, tuple) and self.loss_name.lower() == \"l1\": #Check if it is a tuple, will be this by default when it is fed in.\n",
        "            recon_x = recon_x[0]\n",
        "\n",
        "        #Reconstruction loss\n",
        "        Lrecon = self.recon_loss(x, recon_x, False)\n",
        "        Lrecon = Lrecon.reshape(B, self.no_classes)\n",
        "\n",
        "        if CURL:\n",
        "            Lrecon *= q_y_G_x #weight by categorical likelihood\n",
        "        \n",
        "        Lrecon = torch.sum(Lrecon)  #Sum all values\n",
        "\n",
        "        #continuous KL divergence loss\n",
        "        Lkl_continuous = -1 * self.kl_loss(mu_0, var_0, mu_1, var_1) #Need to normalise with same values reconstruction loss (Pytorch does this automatically unless you specify)\n",
        "        Lkl_continuous = Lkl_continuous.reshape(B, self.no_classes)\n",
        "\n",
        "        Lkl_continuous *= q_y_G_x #weight by categorical likelihood\n",
        "        Lkl_continuous = torch.sum(Lkl_continuous) #Sum all values\n",
        "\n",
        "        #discrete KL divergence loss\n",
        "        Lkl_discrete = -1 * self.discrete_kl_loss(q_y_G_x, prior_prob)\n",
        "        Lkl_discrete = torch.sum(Lkl_discrete)  #Sum all values\n",
        "\n",
        "        Ltotal = self.gamma * Lrecon + self.beta * Lkl_continuous + self.beta * Lkl_discrete\n",
        "\n",
        "        #Normalise value (FIX FOR kB vs B)\n",
        "        Lrecon /= (kB * N)\n",
        "        Lkl_continuous /= (kB * N)\n",
        "        Lkl_discrete /= (kB * N)\n",
        "        Ltotal /= (kB * N)\n",
        "\n",
        "        return Ltotal, Lrecon, Lkl_continuous, Lkl_discrete\n",
        "\n",
        "\"\"\"\n",
        "#TESTING THE FUNCTIONS\n",
        "no_samples = 512\n",
        "no_classes = 3\n",
        "a = torch.randn(no_samples, 5)\n",
        "b = a + torch.randn_like(a) * 0.1\n",
        "mu_0 = torch.ones_like(a)\n",
        "mu_1 = torch.zeros_like(a)\n",
        "var_0 = torch.ones_like(a)\n",
        "var_1 = torch.ones_like(a)\n",
        "\n",
        "a = torch.repeat_interleave(a, no_classes, dim = 0)\n",
        "b = torch.repeat_interleave(b, no_classes, dim = 0)\n",
        "mu_0 = torch.repeat_interleave(mu_0, no_classes, dim = 0)\n",
        "mu_1 = torch.repeat_interleave(mu_1, no_classes, dim = 0)\n",
        "var_0 = torch.repeat_interleave(var_0, no_classes, dim = 0)\n",
        "var_1 = torch.repeat_interleave(var_1, no_classes, dim = 0)\n",
        "\n",
        "loss = MoG_VAE_loss(no_classes)\n",
        "q_y_G_x = torch.ones(no_samples, no_classes) / no_classes\n",
        "\n",
        "print(loss(a, b, mu_0, var_0, mu_1, var_1, q_y_G_x, False))\n",
        "print(loss(a, b, mu_0, var_0, mu_1, var_1, q_y_G_x, True))\n",
        "print(torch.mean((b - a)**2)/2, torch.mean((b - a)**2)/(2 * no_classes))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#TESTING THE FUNCTIONS\\nno_samples = 512\\nno_classes = 3\\na = torch.randn(no_samples, 5)\\nb = a + torch.randn_like(a) * 0.1\\nmu_0 = torch.ones_like(a)\\nmu_1 = torch.zeros_like(a)\\nvar_0 = torch.ones_like(a)\\nvar_1 = torch.ones_like(a)\\n\\na = torch.repeat_interleave(a, no_classes, dim = 0)\\nb = torch.repeat_interleave(b, no_classes, dim = 0)\\nmu_0 = torch.repeat_interleave(mu_0, no_classes, dim = 0)\\nmu_1 = torch.repeat_interleave(mu_1, no_classes, dim = 0)\\nvar_0 = torch.repeat_interleave(var_0, no_classes, dim = 0)\\nvar_1 = torch.repeat_interleave(var_1, no_classes, dim = 0)\\n\\nloss = MoG_VAE_loss(no_classes)\\nq_y_G_x = torch.ones(no_samples, no_classes) / no_classes\\n\\nprint(loss(a, b, mu_0, var_0, mu_1, var_1, q_y_G_x, False))\\nprint(loss(a, b, mu_0, var_0, mu_1, var_1, q_y_G_x, True))\\nprint(torch.mean((b - a)**2)/2, torch.mean((b - a)**2)/(2 * no_classes))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQnjV1THp_iK"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVQtrkYUZeJS"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#Implement similarly to the manner I had previously\n",
        "#Dict to define layers\n",
        "#Checks for FF and Convolution\n",
        "#Add in ability to have variance generating component in decoder (unused at this point)\n",
        "\n",
        "class Unflatten(nn.Module):\n",
        "    def __init__(self, ModelDict):\n",
        "        super(Unflatten, self).__init__()\n",
        "        self.ModelDict = ModelDict\n",
        "        \n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        First_no_channels = self.ModelDict[\"channels\"][0]\n",
        "\n",
        "        input_tensor = input_tensor.view(-1, First_no_channels, int(input_tensor.size(1) / First_no_channels))\n",
        "        \n",
        "        return input_tensor\n",
        "\n",
        "class Flatten(nn.Module): #Same name as tensorflow tf.keras.Flatten()\n",
        "    def __init__(self, DisDict):\n",
        "        super(Flatten, self).__init__()\n",
        "        self.DisDict = DisDict\n",
        "        \n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        input_tensor = input_tensor.view(input_tensor.size(0), -1)\n",
        "        \n",
        "        return input_tensor\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_size, Usize, data_size, encode_dict):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.Usize = Usize\n",
        "        self.data_size = data_size\n",
        "        self.encode_dict = encode_dict\n",
        "        self.activation = nn.LeakyReLU(0.1)\n",
        "        self.var_activation = nn.Softplus()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        #Check if it is a standard VAE through Usize\n",
        "        if self.Usize == 0:\n",
        "            print(\"You are using a MoG VAE but have set the class size to zero... why?\")\n",
        "            raise SystemExit\n",
        "\n",
        "        else:\n",
        "            self.standard_flag = False\n",
        "\n",
        "        self.layers = [] #Initialise layers \n",
        "\n",
        "        if self.encode_dict[\"conv_flag\"]:\n",
        "            \n",
        "            for i in range(len(self.encode_dict[\"channels\"]) - 1):\n",
        "\n",
        "                #append the layer\n",
        "                self.layers.append( nn.Conv1d(in_channels = self.encode_dict[\"channels\"][i], out_channels = self.encode_dict[\"channels\"][i + 1], kernel_size = self.encode_dict[\"kernel_size\"][i], stride = self.encode_dict[\"stride\"][i], padding = self.encode_dict[\"padding\"][i]) )\n",
        "                #append the activation function\n",
        "                self.layers.append(self.activation)\n",
        "            \n",
        "            #append the transform to take the nn.linear to a convolutional layer\n",
        "            self.layers.append(Flatten(self.encode_dict))\n",
        "      \n",
        "        for i in range(len(self.encode_dict[\"ff_layers\"]) - 2):\n",
        "            #append the layer\n",
        "            self.layers.append(nn.Linear(in_features = self.encode_dict[\"ff_layers\"][i], out_features = self.encode_dict[\"ff_layers\"][i + 1], bias = True))\n",
        "            #append the activation function\n",
        "            self.layers.append(self.activation)\n",
        "\n",
        "        self.layers.pop(-1)\n",
        "        self.encode_net = nn.Sequential(*self.layers) #hidden representation that gets fed into predicting the label and latent \n",
        "        self.y_layer = nn.Sequential(nn.Linear(self.encode_dict[\"ff_layers\"][-2], self.Usize), nn.Softmax())\n",
        "\n",
        "        self.mu_layer = nn.Linear(self.encode_dict[\"ff_layers\"][-2] + self.Usize, self.encode_dict[\"ff_layers\"][-1])\n",
        "        self.var_layer = nn.Sequential(nn.Linear(self.encode_dict[\"ff_layers\"][-2] + self.Usize, self.encode_dict[\"ff_layers\"][-1]), self.var_activation)\n",
        "        \n",
        "        self.encode_net.apply(self.init_weights)\n",
        "        self.mu_layer.apply(self.init_weights)\n",
        "        self.var_layer.apply(self.init_weights)\n",
        "    \n",
        "    @staticmethod\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            #m.bias.data.fill_(0.01)\n",
        "  \n",
        "    def forward(self, x, cont_input = None, train_flag = True, mode_labels = True):\n",
        "        \n",
        "        #Always stack as [x, conditional labels]\n",
        "        #mode_labels specifies whether to use softmax outputs for forward generation or whether they are one-hot encoded\n",
        "        #When you train, you specify the class exactly (you do not use self.y_layer to compute y|x, but rather expand the data to account for L labels and recompute z)\n",
        "\n",
        "        if cont_input is not None and not self.standard_flag:\n",
        "            x_input = torch.hstack((x, cont_input))\n",
        "\n",
        "        else:\n",
        "            x_input = x\n",
        "\n",
        "        encode = self.encode_net(x_input)\n",
        "        y_x = self.y_layer(encode)\n",
        "        y_labels = torch.argmax(y_x)\n",
        "\n",
        "        if not self.CURL_flag:\n",
        "            mu_z = self.mu_layer(encode)\n",
        "            var_z = self.var_layer(encode)\n",
        "            return mu_z, var_z, y_labels\n",
        "\n",
        "        if train_flag: \n",
        "            \n",
        "            with torch.no_grad():\n",
        "                x_input = torch.repeat_interleave(x_input, self.Usize, dim = 0)\n",
        "                labels = torch.repeat_interleave(torch.arange(self.Usize), (x_input.size(0),)) #Repeat labels for each sample\n",
        "\n",
        "        if mode_labels and self.CURL_flag: #CURL adds conditions Z on X and Y, specifies that you \n",
        "            u_input = F.one_hot(labels, num_classes = self.Usize)\n",
        "            encode = torch.hstack((encode, u_input))\n",
        "\n",
        "        elif not mode_labels and self.CURL_flag: \n",
        "            encode = torch.hstack((encode, y_x))\n",
        "\n",
        "        mu_z = self.mu_layer(encode)\n",
        "        var_z = self.var_layer(encode)\n",
        "\n",
        "        return mu_z, var_z, y_x #y_x is a probability (technically)\n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_size, Usize, data_size, decode_dict, var_flag = False):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.Usize = Usize\n",
        "        self.data_size = data_size\n",
        "        self.decode_dict = decode_dict\n",
        "        self.var_flag = var_flag\n",
        "        self.activation = nn.LeakyReLU(0.1)    \n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.layers = [] #Initialise layers \n",
        "        \n",
        "        if not self.decode_dict[\"conv_flag\"]:\n",
        "            for i in range(len(self.decode_dict[\"ff_layers\"]) - 2):\n",
        "                #append the layer\n",
        "                self.layers.append(nn.Linear(in_features = self.decode_dict[\"ff_layers\"][i], out_features = self.decode_dict[\"ff_layers\"][i + 1], bias = True))\n",
        "                #append the activation function\n",
        "                self.layers.append(self.activation)\n",
        "            \n",
        "            self.layers.pop(-1) #remove the final activation for linear outputs\n",
        "    \n",
        "            self.decode_net = nn.Sequential(*self.layers)\n",
        "            self.gen_layer = nn.Linear(self.decode_dict[\"ff_layers\"][-2], self.decode_dict[\"ff_layers\"][-1])\n",
        "            \n",
        "            if self.var_flag:\n",
        "                self.var_layer = nn.Sequential(nn.Linear(self.decode_dict[\"ff_layers\"][-2], self.decode_dict[\"ff_layers\"][-1]), nn.Softplus())\n",
        "                #self.var_layer.apply(self.init_weights)\n",
        "        \n",
        "         \n",
        "        else:\n",
        "            for i in range(len(self.decode_dict[\"ff_layers\"]) - 1):\n",
        "                #append the layer\n",
        "                self.layers.append(nn.Linear(in_features = self.decode_dict[\"ff_layers\"][i], out_features = self.decode_dict[\"ff_layers\"][i + 1], bias = True))\n",
        "                #append the activation function\n",
        "                self.layers.append(self.activation)\n",
        "        \n",
        "            #append the transform to take the nn.linear to a convolutional layer\n",
        "            self.layers.append(Unflatten(self.decode_dict))\n",
        "            \n",
        "            for i in range(len(self.decode_dict[\"channels\"]) - 2):\n",
        "\n",
        "                #append the layer\n",
        "                self.layers.append( nn.ConvTranspose1d(in_channels = self.decode_dict[\"channels\"][i], out_channels = self.decode_dict[\"channels\"][i + 1], kernel_size = self.decode_dict[\"kernel_size\"][i], stride = self.decode_dict[\"stride\"][i], padding = self.decode_dict[\"padding\"][i]) )\n",
        "                #append the activation function\n",
        "                self.layers.append(self.activation)\n",
        "        \n",
        "            self.layers.pop(-1) #remove the final activation for linear outputs\n",
        "    \n",
        "            self.decode_net = nn.Sequential(*self.layers)\n",
        "            self.gen_layer = nn.ConvTranspose1d(in_channels = self.decode_dict[\"channels\"][-2], out_channels = self.decode_dict[\"channels\"][-1], kernel_size = self.decode_dict[\"kernel_size\"][-1], stride = self.decode_dict[\"stride\"][-1], padding = self.decode_dict[\"padding\"][-1])\n",
        "\n",
        "            #self.decode_net.apply(self.init_weights)\n",
        "            #self.gen_layer.apply(self.init_weights)\n",
        "\n",
        "            if self.var_flag:\n",
        "                self.var_layer = nn.Sequential(nn.ConvTranspose1d(in_channels = self.decode_dict[\"channels\"][-2], out_channels = self.decode_dict[\"channels\"][-1], kernel_size = self.decode_dict[\"kernel_size\"][-1], stride = self.decode_dict[\"stride\"][-1], padding = self.decode_dict[\"padding\"][-1])\n",
        "                                               , nn.Softplus())\n",
        "                #self.var_layer.apply(self.init_weights)\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            #m.bias.data.fill_(0.01)\n",
        "\n",
        "    @staticmethod\n",
        "    def reparametrisation_trick(mu_data, var_data):\n",
        "        with torch.no_grad():\n",
        "            eta = torch.randn_like(mu_data)\n",
        "\n",
        "        return mu_data + eta * torch.sqrt(var_data)\n",
        "\n",
        "    def forward(self, mu_latent, var_latent):\n",
        "\n",
        "        z_latent = self.reparametrisation_trick(mu_latent, var_latent)\n",
        "\n",
        "        decode_out = self.decode_net(z_latent)\n",
        "\n",
        "        x_out = self.gen_layer(decode_out)\n",
        "\n",
        "        if self.var_flag:\n",
        "            var_out = self.var_layer(decode_out)\n",
        "            \n",
        "        else:\n",
        "            var_out = torch.ones_like(x_out).requires_grad_(False)\n",
        "        \n",
        "        if self.decode_dict[\"conv_flag\"]:\n",
        "            x_out = x_out.squeeze(1)\n",
        "            var_out = var_out.squeeze(1)\n",
        "            \n",
        "        return x_out, var_out\n",
        "\n",
        "        \n",
        "  \n",
        "class ConditionalPrior(nn.Module):\n",
        "    #Can adapt to have parametric densities... (only a mean and variance parameter depending on the class)\n",
        "    def __init__(self, latent_size, Usize, data_size, prior_dict, continuous_prior = True):\n",
        "        super(ConditionalPrior, self).__init__()\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.Usize = Usize\n",
        "        self.data_size = data_size\n",
        "        self.prior_dict = prior_dict\n",
        "        self.continuous_prior = continuous_prior\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.1)\n",
        "        self.var_activation = nn.Softplus()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        #Check if it is a standard VAE, if so, set continuous_prior to False and then set distribution to N(0, I)\n",
        "        if self.Usize == 0:\n",
        "            self.continuous_prior = False\n",
        "\n",
        "        self.layers = [] #Initialise layers \n",
        "        \n",
        "        if self.continuous_prior:\n",
        "            #Define model - essentially another generator but with only FF layers, by design\n",
        "\n",
        "            for i in range(len(self.prior_dict[\"ff_layers\"]) - 2):\n",
        "                #append the layer\n",
        "                self.layers.append(nn.Linear(in_features = self.prior_dict[\"ff_layers\"][i], out_features = self.prior_dict[\"ff_layers\"][i + 1], bias = True))\n",
        "                #append the activation function\n",
        "                self.layers.append(self.activation)\n",
        "          \n",
        "            self.layers.pop(-1)\n",
        "            self.prior_net = nn.Sequential(*self.layers)\n",
        "            self.prior_mu = nn.Linear(self.prior_dict[\"ff_layers\"][-2], self.prior_dict[\"ff_layers\"][-1])\n",
        "            self.prior_var = nn.Linear(self.prior_dict[\"ff_layers\"][-2], self.prior_dict[\"ff_layers\"][-1])\n",
        "\n",
        "            #self.prior_net.apply(self.init_weights)\n",
        "            #self.prior_mu.apply(self.init_weights)\n",
        "            #self.prior_var.apply(self.init_weights)\n",
        "      \n",
        "        else:\n",
        "            #Lambda functions that just return the mean and variance parameters at all the class locations of interest!\n",
        "\n",
        "            self.prior_net = lambda U: U\n",
        "            \n",
        "            \n",
        "            if self.Usize == 0:\n",
        "                #self._prior_mu_ = nn.parameter.Parameter(torch.Tensor(1, self.latent_size))\n",
        "                #self._prior_var_ = nn.parameter.Parameter(torch.Tensor(1, self.latent_size))\n",
        "                self.register_parameter(name='_prior_mu_', param=torch.nn.Parameter(torch.Tensor(1, self.latent_size)))\n",
        "                self.register_parameter(name='_prior_var_', param=torch.nn.Parameter(torch.Tensor(1, self.latent_size)))\n",
        "\n",
        "            else:\n",
        "                self.register_parameter(name='_prior_mu_', param=torch.nn.Parameter(torch.Tensor(self.Usize, self.latent_size)))\n",
        "                self.register_parameter(name='_prior_var_', param=torch.nn.Parameter(torch.Tensor(self.Usize, self.latent_size)))\n",
        "                #self._prior_mu_ = nn.parameter.Parameter(torch.Tensor(self.Usize, self.latent_size))\n",
        "                #self._prior_var_ = nn.parameter.Parameter(torch.Tensor(self.Usize, self.latent_size))#torch.ones(self.Usize, self.latent_size).to(self.device)#\n",
        "\n",
        "                self.prior_mu = lambda U: self._prior_mu_[torch.argmax(U, dim = 1), :]\n",
        "                self.prior_var = lambda U: self._prior_var_[torch.argmax(U, dim = 1), :]\n",
        "\n",
        "            with torch.no_grad(): #initialise parameters\n",
        "                if self.Usize == 0:\n",
        "                    #Set to N(0, I)\n",
        "                    self._prior_mu_.fill_(0)\n",
        "                    self._prior_var_.fill_(1)\n",
        "                    #Turn off gradient flag\n",
        "                    self._prior_mu_.requires_grad_(False)\n",
        "                    self._prior_var_.requires_grad_(False)\n",
        "                    \n",
        "                else:\n",
        "                    self._prior_mu_.normal_(0, 0.1)\n",
        "                    self._prior_var_.normal_(0, 0.1)\n",
        "\n",
        "    @staticmethod\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            #m.bias.data.fill_(0.01)\n",
        "    \n",
        "    def one_hot_encode(self, labels):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            label_mat = torch.zeros(labels.size(0), self.Usize)\n",
        "            label_mat[range(labels.size(0)), labels] = 1\n",
        "\n",
        "            return label_mat\n",
        "    \n",
        "    def forward(self, labels = None, cont_input = None):\n",
        "        #Always stack as [continuous, discrete]\n",
        "        if self.Usize == 0:\n",
        "            return self._prior_mu_, self._prior_var_\n",
        "\n",
        "        else:\n",
        "            #if self.continuous_prior:\n",
        "            #    u_input = self.one_hot_encode(labels)\n",
        "            \n",
        "            #else:\n",
        "            u_input = labels\n",
        "            \n",
        "            \n",
        "            if cont_input is not None:\n",
        "                u_input = torch.hstack((cont_input, u_input))\n",
        "\n",
        "            prior_net = self.prior_net(u_input)\n",
        "            mu = self.prior_mu(prior_net)\n",
        "            var = self.var_activation(self.prior_var(prior_net)) \n",
        "            \n",
        "            return mu, var\n",
        "\n",
        "class VAE_model(nn.Module):\n",
        "    def __init__(self, input_size, latent_size, U_size = None, EncodeDict = None, DecodeDict = None, PriorDict = None, var_decode = False, continuous_prior = True):\n",
        "        super(VAE_model, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.latent_size = latent_size\n",
        "        self.U_size = U_size\n",
        "        self.encode_dict = EncodeDict\n",
        "        self.decode_dict = DecodeDict\n",
        "        self.prior_dict = PriorDict\n",
        "        self.var_decode = var_decode\n",
        "        self.continuous_prior = continuous_prior\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.model_HI_names = [\"HI_1\"]\n",
        "        self.model_HI_names_pretty = [r\"$NLL_{recon}$\"]\n",
        "        \n",
        "        #self.U_size\n",
        "        self.encoder = Encoder(self.latent_size, 0, self.input_size, self.encode_dict)\n",
        "        self.decoder = Decoder(self.latent_size, self.U_size, self.input_size, self.decode_dict, var_flag = var_decode)\n",
        "        self.prior = ConditionalPrior(self.latent_size, self.U_size, self.input_size, self.prior_dict, continuous_prior = self.continuous_prior)\n",
        "\n",
        "        if self.U_size == 0:\n",
        "            print(\"\\nInitialising a normal VAE!\\n\")\n",
        "            self.standard_flag = True\n",
        "        \n",
        "        else:\n",
        "            self.standard_flag = False \n",
        "  \n",
        "    def train(self):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "        self.prior.train()\n",
        "    \n",
        "    def eval(self):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "        self.prior.eval()\n",
        "\n",
        "    def to(self, device):\n",
        "        self.encoder.to(device)\n",
        "        self.decoder.to(device)\n",
        "        self.prior.to(device)\n",
        "    \n",
        "    def one_hot_encode(self, labels):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            label_mat = torch.zeros(labels.size(0), self.Usize)\n",
        "            label_mat[range(labels.size(0)), labels] = 1\n",
        "\n",
        "            return label_mat\n",
        "\n",
        "    def compute_HIs(self, x, labels = None, cont_input = None): #Only useful if you are performing anomaly detection (specific to another project)\n",
        "        with torch.no_grad():\n",
        "\n",
        "            mu_latent, var_latent = self.encoder(x, labels, cont_input)\n",
        "\n",
        "            x_recon1, var_decoder =  self.decoder(mu_latent, var_latent) \n",
        "            HI_1 = (1 / x.shape[1]) * torch.sum((x - x_recon1)**2 / (var_decoder), dim = 1) \n",
        "\n",
        "            return HI_1, mu_latent\n",
        "\n",
        "class VAE_optimiser(object):\n",
        "    def __init__(self, model, Params):\n",
        "        ls = list(model.encoder.parameters()) + list(model.decoder.parameters()) + list(model.prior.parameters())\n",
        "        self.VAE_opt = torch.optim.Adam(ls, lr = Params.learning_rate)\n",
        "    \n",
        "    def step(self):\n",
        "        self.VAE_opt.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.VAE_opt.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpqFzU_31qgg",
        "outputId": "62c3858f-f08e-45bb-f68d-f0e2d5991892"
      },
      "source": [
        "torch.tile(torch.arange(5), (32,)) #Repeat labels for each sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
              "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
              "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
              "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
              "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
              "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
              "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NALsqcdYvbLq",
        "outputId": "d78ac620-b678-4fcb-908d-72c596428e2d"
      },
      "source": [
        "a = torch.randint(0, 4, (10, 4))\n",
        "print(a)\n",
        "torch.argmax(a, dim = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3, 0, 2, 1],\n",
            "        [3, 0, 1, 0],\n",
            "        [1, 0, 1, 0],\n",
            "        [2, 0, 1, 2],\n",
            "        [3, 2, 1, 2],\n",
            "        [3, 0, 0, 0],\n",
            "        [3, 0, 2, 2],\n",
            "        [2, 2, 0, 3],\n",
            "        [2, 1, 2, 3],\n",
            "        [0, 1, 0, 1]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 3, 3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2tn2QdqxK0z",
        "outputId": "e085e135-34aa-4b77-c24a-fb1615dfe5ca"
      },
      "source": [
        "import scipy.optimize as optimize\n",
        "\n",
        "no_classes = 5\n",
        "x = np.ones(no_classes) * 0.01\n",
        "\n",
        "\n",
        "def cost(x):\n",
        "  u = np.ones(no_classes) / no_classes\n",
        "  return -1 * np.sum(x * np.log(u / x))\n",
        "\n",
        "def constraint(x):\n",
        "  return np.sum(x) - 1\n",
        "\n",
        "opt_dict = optimize.minimize(cost, x, constraints = {'type':'eq', 'fun': constraint})\n",
        "\n",
        "print(opt_dict['x'])\n",
        "print(1/6, 1/0.07357588)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2 0.2 0.2 0.2 0.2]\n",
            "0.16666666666666666 13.591410663385883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piIdDzBG7KgI",
        "outputId": "118424ca-6ca9-4ff7-8b01-394c587ee586"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "output = loss(input, target)\n",
        "\n",
        "print(target, output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 0, 3]) tensor(2.3567, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}